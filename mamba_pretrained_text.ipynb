{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, MambaForCausalLM\n",
    "from datasets import load_dataset\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Trelis/tiny-shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (472, 1), 'test': (49, 1)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text'],\n",
       "    num_rows: 472\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TRANIO:\\nIs this your speeding? nay, then, good night our part!\\n\\nPETRUCHIO:\\nBe patient, gentlemen; I choose her for myself:\\nIf she and I be pleased, what's that to you?\\n'Tis bargain'd 'twixt us twain, being alone,\\nThat she shall still be curst in company.\\nI tell you, 'tis incredible to believe\\nHow much she loves me: O, the kindest Kate!\\nShe hung about my neck; and kiss on kiss\\nShe vied so fast, protesting oath on oath,\\nThat in a twink she won me to her love.\\nO, you are novices! 'tis a world to see,\\nHow tame, when men and women are alone,\\nA meacock wretch can make the curstest shrew.\\nGive me thy hand, Kate: I will unto Venice,\\nTo buy apparel 'gainst the wedding-day.\\nProvide the feast, father, and bid the guests;\\nI will be sure my Katharina shall be fine.\\n\\nBAPTISTA:\\nI know not what to say: but give me your hands;\\nGod send you joy, Petruchio! 'tis a match.\\n\\nGREMIO:\\nAmen, say we: we will be witnesses.\\n\\nPETRUCHIO:\\nFather, and wife, and gentlemen, adieu;\\nI will to Venice; Sunday comes apace:\\nWe will have rings and things and fine array;\\nAnd kiss me, Kate, we will be married o'Sunday.\\n\\nGREMIO:\\nWas ever match clapp'd up so suddenly?\\n\\nBAPTISTA:\\nFaith, gentlemen, now I play a merchant's part,\\nAnd venture madly on a desperate mart.\\n\\nTRANIO:\\n'Twas a commodity lay fretting by you:\\n'Twill bring you gain, or perish on the seas.\\n\\nBAPTISTA:\\nThe gain I seek is, quiet in the match.\\n\\nGREMIO:\\nNo doubt but he hath got a quiet catch.\\nBut now, Baptists, to your younger daughter:\\nNow is the day we long have looked for:\\nI am your neighbour, and was suitor first.\\n\\nTRANIO:\\nAnd I am one that love Bianca more\\nThan words can witness, or your thoughts can guess.\\n\\nGREMIO:\\nYoungling, thou canst not love so dear as I.\\n\\nTRANIO:\\nGraybeard, thy love doth freeze.\\n\\nGREMIO:\\nBut thine doth fry.\\nSkipper, stand back: 'tis age that nourisheth.\\n\\nTRANIO:\\nBut youth in ladies' eyes that flourisheth.\\n\\nBAPTISTA:\\nContent you, gentlemen: I will compound this strife:\\n'Tis deeds must win the prize; and he of both\\nThat can assure my daughter greatest dower\\nShall have my Bianca's love.\\nSay, Signior Gremio, What can you assure her?\\n\\nGREMIO:\\nFirst, as you know, my house within the city\\nIs richly furnished with plate and gold;\\nBasins and ewers to lave her dainty hands;\\nMy hangings all of Tyrian tapestry;\\nIn ivory coffers I have stuff'd my crowns;\\nIn cypress chests my arras counterpoints,\\nCostly apparel, tents, and canopies,\\nFine linen, Turkey cushions boss'd with pearl,\\nValance of Venice gold in needlework,\\nPewter and brass and all things that belong\\nTo house or housekeeping: then, at my farm\\nI have a hundred milch-kine to the pail,\\nSixscore fat oxen standing in my stalls,\\nAnd all things answerable to this portion.\\nMyself am struck in years, I must confess;\\nAnd if I die to-morrow, this is hers,\\nIf whilst I live she will be only mine.\\n\\nTRANIO:\\nThat 'only' came well in.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2859"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"test\"][\"Text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the naive implementation. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d\n"
     ]
    }
   ],
   "source": [
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(dataset[\"test\"][\"Text\"], return_tensors= \"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itrujillo/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'architectures': ['MambaForCausalLM'],\n",
       "  'bos_token_id': 0,\n",
       "  'conv_kernel': 4,\n",
       "  'd_inner': 160,\n",
       "  'd_model': 1024,\n",
       "  'dt_rank': 'auto',\n",
       "  'eos_token_id': 0,\n",
       "  'expand': 2,\n",
       "  'fused_add_norm': True,\n",
       "  'hidden_act': 'silu',\n",
       "  'hidden_size': 1024,\n",
       "  'initializer_range': 0.1,\n",
       "  'intermediate_size': 2048,\n",
       "  'layer_norm_epsilon': 1e-05,\n",
       "  'model_type': 'mamba',\n",
       "  'n_layer': 48,\n",
       "  'num_hidden_layers': 48,\n",
       "  'pad_token_id': 0,\n",
       "  'pad_vocab_size_multiple': 8,\n",
       "  'rescale_prenorm_residual': False,\n",
       "  'residual_in_fp32': True,\n",
       "  'rms_norm': True,\n",
       "  'ssm_cfg': {},\n",
       "  'state_size': 16,\n",
       "  'time_step_floor': 0.0001,\n",
       "  'time_step_init_scheme': 'random',\n",
       "  'time_step_max': 0.1,\n",
       "  'time_step_min': 0.001,\n",
       "  'time_step_rank': 64,\n",
       "  'time_step_scale': 1.0,\n",
       "  'torch_dtype': 'float32',\n",
       "  'transformers_version': '4.39.0.dev0',\n",
       "  'use_bias': False,\n",
       "  'use_cache': True,\n",
       "  'use_conv_bias': True,\n",
       "  'vocab_size': 50280,\n",
       "  '_commit_hash': 'b519127f5bfaaa1c27dd938dad051ec360972b23'},\n",
       " {})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.get_config_dict(\"state-spaces/mamba-370m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    input_ids = encodings.input_ids.to(device)\n",
    "    outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MambaCausalLMOutput(loss=None, logits=tensor([[[  5.0041,  -5.3653,   4.8237,  ...,  -5.4267,  -5.4574,  -5.4511],\n",
      "         [  2.5265,  -5.9306,   4.7795,  ...,  -5.8653,  -5.9365,  -6.1129],\n",
      "         [  8.4780,  -0.7837,   8.8056,  ...,  -0.8313,  -0.8446,  -0.9851],\n",
      "         ...,\n",
      "         [ 23.1578,   7.7686,  24.0983,  ...,   7.6455,   7.6588,   7.3878],\n",
      "         [ 23.1557,   7.7641,  24.0947,  ...,   7.6411,   7.6543,   7.3834],\n",
      "         [ 23.1535,   7.7597,  24.0911,  ...,   7.6368,   7.6500,   7.3790]],\n",
      "\n",
      "        [[ -1.5431, -16.3554,   3.8924,  ..., -16.5352, -16.3415, -16.4642],\n",
      "         [-30.1016, -43.8163, -32.0511,  ..., -43.7683, -43.7739, -43.4251],\n",
      "         [-29.4698, -42.7764, -24.9282,  ..., -42.6009, -42.3791, -42.2224],\n",
      "         ...,\n",
      "         [ 23.8008,   9.0419,  24.9545,  ...,   8.8931,   8.9215,   8.6503],\n",
      "         [ 23.7399,   8.9403,  24.8939,  ...,   8.7936,   8.8207,   8.5497],\n",
      "         [ 23.7064,   8.8789,  24.8581,  ...,   8.7337,   8.7599,   8.4893]],\n",
      "\n",
      "        [[-25.3489, -37.9731, -23.3294,  ..., -37.9446, -37.8954, -37.8251],\n",
      "         [-31.4593, -43.2288, -33.0063,  ..., -43.1930, -43.2536, -42.8974],\n",
      "         [-28.4309, -37.7938, -26.1109,  ..., -37.6267, -37.5905, -37.2472],\n",
      "         ...,\n",
      "         [ 23.0230,   7.4350,  23.8303,  ...,   7.3219,   7.3311,   7.0559],\n",
      "         [ 23.0229,   7.4349,  23.8301,  ...,   7.3217,   7.3309,   7.0558],\n",
      "         [ 23.0227,   7.4347,  23.8301,  ...,   7.3215,   7.3308,   7.0556]],\n",
      "\n",
      "        [[-33.2551, -46.1432, -27.3710,  ..., -46.0915, -46.0338, -45.8660],\n",
      "         [-32.3809, -43.4995, -32.7568,  ..., -43.4580, -43.4558, -43.2175],\n",
      "         [  2.8212,  -6.7201,   6.4297,  ...,  -6.7520,  -6.7358,  -6.8894],\n",
      "         ...,\n",
      "         [-16.8989, -30.4054,  -7.8987,  ..., -30.4356, -30.4698, -30.3434],\n",
      "         [ 34.9444,  23.3791,  46.6563,  ...,  23.2464,  23.2060,  23.2350],\n",
      "         [ 37.6146,  22.2719,  35.7159,  ...,  21.9634,  22.0866,  21.8857]],\n",
      "\n",
      "        [[  5.0041,  -5.3653,   4.8237,  ...,  -5.4267,  -5.4574,  -5.4511],\n",
      "         [  2.5265,  -5.9306,   4.7795,  ...,  -5.8653,  -5.9365,  -6.1129],\n",
      "         [  8.4780,  -0.7837,   8.8056,  ...,  -0.8313,  -0.8446,  -0.9851],\n",
      "         ...,\n",
      "         [ 30.7914,  23.8055,  36.8869,  ...,  23.5285,  23.7378,  23.5841],\n",
      "         [ 25.1116,  16.1726,  26.7462,  ...,  15.8716,  16.0641,  15.8836],\n",
      "         [ 23.0196,   7.4044,  23.7293,  ...,   7.2890,   7.2913,   7.0081]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd0a0b1cfa0>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[-25.3489, -37.9731, -23.3294,  ..., -37.9446, -37.8954, -37.8251],\n",
      "         [-24.2770, -36.2974, -18.5093,  ..., -36.1590, -36.3921, -36.0720],\n",
      "         [-14.1547, -27.1201, -11.1573,  ..., -26.7380, -26.7420, -26.8234],\n",
      "         ...,\n",
      "         [ 23.0252,   7.4437,  23.8368,  ...,   7.3302,   7.3397,   7.0645],\n",
      "         [ 23.0252,   7.4434,  23.8366,  ...,   7.3299,   7.3394,   7.0643],\n",
      "         [ 23.0250,   7.4431,  23.8365,  ...,   7.3296,   7.3391,   7.0640]],\n",
      "\n",
      "        [[  8.4800,  -4.9714,  10.1019,  ...,  -4.9356,  -4.8326,  -5.0413],\n",
      "         [-18.9771, -32.5636, -19.3957,  ..., -32.4358, -32.3688, -32.1871],\n",
      "         [-22.9363, -39.9659, -19.0717,  ..., -39.9803, -39.9339, -39.6694],\n",
      "         ...,\n",
      "         [  4.0370,  -5.3429,   8.3027,  ...,  -5.2590,  -5.3732,  -5.4381],\n",
      "         [ 15.9314,   6.9525,  18.3086,  ...,   6.8319,   6.7486,   6.7225],\n",
      "         [ 32.5969,  18.4625,  38.1002,  ...,  18.5310,  18.3190,  18.0031]],\n",
      "\n",
      "        [[-32.3283, -45.3312, -29.2714,  ..., -45.1931, -45.3169, -44.9916],\n",
      "         [-26.1937, -37.5124, -26.3322,  ..., -37.1431, -37.2309, -36.9176],\n",
      "         [ 12.9579,   1.9678,  15.5448,  ...,   1.8495,   1.7386,   1.7892],\n",
      "         ...,\n",
      "         [ 23.1701,   7.7989,  24.1229,  ...,   7.6756,   7.6891,   7.4180],\n",
      "         [ 23.1676,   7.7938,  24.1187,  ...,   7.6706,   7.6841,   7.4129],\n",
      "         [ 23.1651,   7.7887,  24.1147,  ...,   7.6657,   7.6791,   7.4079]],\n",
      "\n",
      "        [[ 13.9041,   2.2270,  12.4691,  ...,   2.2358,   2.3350,   2.1661],\n",
      "         [  4.1180, -12.2675,   5.1765,  ..., -12.4086, -12.0110, -12.3777],\n",
      "         [ -1.1443, -19.5567,  -0.9869,  ..., -19.6048, -19.5797, -19.7136],\n",
      "         ...,\n",
      "         [ 23.0639,   7.5421,  23.9113,  ...,   7.4247,   7.4356,   7.1624],\n",
      "         [ 23.0636,   7.5410,  23.9105,  ...,   7.4237,   7.4345,   7.1613],\n",
      "         [ 23.0633,   7.5400,  23.9097,  ...,   7.4226,   7.4334,   7.1602]],\n",
      "\n",
      "        [[-32.3283, -45.3312, -29.2714,  ..., -45.1931, -45.3169, -44.9916],\n",
      "         [ -4.5134, -12.0138,  -1.9110,  ..., -11.7893, -11.7505, -11.9709],\n",
      "         [ 18.8362,   8.9960,  25.2338,  ...,   8.9600,   8.8907,   8.8714],\n",
      "         ...,\n",
      "         [ 22.9490,   7.2909,  23.6322,  ...,   7.1842,   7.1815,   6.8976],\n",
      "         [ 24.7992,  10.5600,  25.8416,  ...,  10.3776,  10.4232,  10.1574],\n",
      "         [ 23.8667,   9.1327,  25.0139,  ...,   8.9844,   9.0114,   8.7395]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd0a1ca6fd0>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[-32.3283, -45.3312, -29.2714,  ..., -45.1931, -45.3169, -44.9916],\n",
      "         [-23.0510, -36.1568, -19.1596,  ..., -35.8220, -36.1829, -35.6540],\n",
      "         [-21.5783, -34.0887, -13.5675,  ..., -33.8654, -34.2128, -33.6610],\n",
      "         ...,\n",
      "         [ 23.1234,   7.6869,  24.0274,  ...,   7.5654,   7.5777,   7.3065],\n",
      "         [ 23.1220,   7.6840,  24.0250,  ...,   7.5625,   7.5748,   7.3036],\n",
      "         [ 23.1208,   7.6811,  24.0226,  ...,   7.5597,   7.5720,   7.3008]],\n",
      "\n",
      "        [[ -3.1600, -17.4955,  -2.6386,  ..., -17.6022, -17.5901, -17.6228],\n",
      "         [  8.3502,  -0.9672,  12.7584,  ...,  -1.4151,  -1.3387,  -0.8616],\n",
      "         [-26.5092, -37.5321, -25.4983,  ..., -37.6014, -37.4932, -37.2034],\n",
      "         ...,\n",
      "         [ 23.6445,   8.7508,  24.7834,  ...,   8.6084,   8.6330,   8.3629],\n",
      "         [ 23.6128,   8.6876,  24.7441,  ...,   8.5466,   8.5703,   8.3003],\n",
      "         [ 23.5845,   8.6301,  24.7079,  ...,   8.4902,   8.5132,   8.2433]],\n",
      "\n",
      "        [[-16.1015, -30.1359, -14.8562,  ..., -30.1392, -29.8177, -29.8586],\n",
      "         [ -9.5816, -18.0177,  -7.5788,  ..., -17.7484, -17.9062, -17.8425],\n",
      "         [-27.1018, -39.1130, -23.3841,  ..., -38.6981, -38.9347, -38.7653],\n",
      "         ...,\n",
      "         [  8.3773,   5.7598,  12.2583,  ...,   5.7033,   5.4143,   5.5037],\n",
      "         [ -6.4580, -18.4491,   2.4302,  ..., -18.4463, -18.5518, -18.6067],\n",
      "         [ -9.0961, -21.1496, -13.1975,  ..., -21.3504, -21.1652, -21.2735]],\n",
      "\n",
      "        [[  5.0041,  -5.3653,   4.8237,  ...,  -5.4267,  -5.4574,  -5.4511],\n",
      "         [  2.5265,  -5.9306,   4.7795,  ...,  -5.8653,  -5.9365,  -6.1129],\n",
      "         [  8.4780,  -0.7837,   8.8056,  ...,  -0.8313,  -0.8446,  -0.9851],\n",
      "         ...,\n",
      "         [ 22.9954,   7.4128,  23.8174,  ...,   7.3021,   7.3108,   7.0343],\n",
      "         [ 22.9953,   7.4127,  23.8173,  ...,   7.3020,   7.3107,   7.0342],\n",
      "         [ 22.9952,   7.4127,  23.8173,  ...,   7.3019,   7.3106,   7.0342]],\n",
      "\n",
      "        [[-28.2829, -41.7650, -23.9903,  ..., -41.5811, -41.6923, -41.4938],\n",
      "         [-25.0811, -40.3224, -19.8450,  ..., -39.9091, -40.0940, -39.8857],\n",
      "         [-20.5094, -36.0471, -14.3536,  ..., -35.6524, -35.9378, -35.8130],\n",
      "         ...,\n",
      "         [ 23.0284,   7.4527,  23.8445,  ...,   7.3387,   7.3484,   7.0734],\n",
      "         [ 23.0283,   7.4524,  23.8443,  ...,   7.3384,   7.3481,   7.0731],\n",
      "         [ 23.0281,   7.4520,  23.8440,  ...,   7.3380,   7.3477,   7.0727]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd19938af40>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[-28.2829, -41.7650, -23.9903,  ..., -41.5811, -41.6923, -41.4938],\n",
      "         [-25.0811, -40.3224, -19.8450,  ..., -39.9091, -40.0940, -39.8857],\n",
      "         [-20.5094, -36.0471, -14.3536,  ..., -35.6524, -35.9378, -35.8130],\n",
      "         ...,\n",
      "         [ 23.5212,   8.5115,  24.6304,  ...,   8.3746,   8.3957,   8.1258],\n",
      "         [ 23.5002,   8.4678,  24.6023,  ...,   8.3318,   8.3524,   8.0825],\n",
      "         [ 23.4806,   8.4274,  24.5761,  ...,   8.2922,   8.3123,   8.0425]],\n",
      "\n",
      "        [[-32.5125, -44.3043, -30.7945,  ..., -44.0737, -44.1203, -43.8588],\n",
      "         [-15.6522, -26.2626, -11.4820,  ..., -26.2923, -26.2802, -26.0752],\n",
      "         [-27.2573, -38.8636, -26.9572,  ..., -38.7750, -38.8508, -38.6054],\n",
      "         ...,\n",
      "         [ 28.1467,  18.3877,  32.1400,  ...,  18.3292,  18.1751,  18.2837],\n",
      "         [ -7.1897, -14.4519,   2.4848,  ..., -14.2610, -14.5617, -14.4685],\n",
      "         [-13.8035, -21.4898, -14.6789,  ..., -21.6754, -21.6057, -21.7070]],\n",
      "\n",
      "        [[  1.3981,  -8.1870,   1.9304,  ...,  -8.0798,  -8.2595,  -8.3618],\n",
      "         [  3.6575,  -8.9830,   4.8426,  ...,  -8.9504,  -8.7884,  -9.3692],\n",
      "         [  0.5569, -10.0765,   1.7060,  ..., -10.0141, -10.0683, -10.2498],\n",
      "         ...,\n",
      "         [ 22.9958,   7.4120,  23.8159,  ...,   7.3012,   7.3099,   7.0335],\n",
      "         [ 22.9957,   7.4119,  23.8158,  ...,   7.3012,   7.3099,   7.0334],\n",
      "         [ 22.9956,   7.4119,  23.8158,  ...,   7.3011,   7.3098,   7.0333]],\n",
      "\n",
      "        [[-20.3344, -33.2924, -18.3172,  ..., -33.2013, -33.0098, -33.0764],\n",
      "         [-31.2728, -44.6135, -24.8497,  ..., -44.6656, -44.5069, -44.2709],\n",
      "         [-14.9165, -30.1751, -13.8509,  ..., -29.9833, -29.9343, -29.8975],\n",
      "         ...,\n",
      "         [ 23.0410,   7.4677,  23.8549,  ...,   7.3528,   7.3627,   7.0883],\n",
      "         [ 23.0408,   7.4673,  23.8546,  ...,   7.3524,   7.3623,   7.0879],\n",
      "         [ 23.0406,   7.4668,  23.8542,  ...,   7.3520,   7.3619,   7.0875]],\n",
      "\n",
      "        [[-20.3344, -33.2924, -18.3172,  ..., -33.2013, -33.0098, -33.0764],\n",
      "         [-31.2728, -44.6135, -24.8497,  ..., -44.6656, -44.5069, -44.2709],\n",
      "         [-14.9165, -30.1751, -13.8509,  ..., -29.9833, -29.9343, -29.8975],\n",
      "         ...,\n",
      "         [ 23.1402,   7.7288,  24.0640,  ...,   7.6067,   7.6194,   7.3483],\n",
      "         [ 23.1384,   7.7250,  24.0609,  ...,   7.6030,   7.6158,   7.3446],\n",
      "         [ 23.1367,   7.7213,  24.0579,  ...,   7.5993,   7.6120,   7.3408]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd19937e670>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[ 1.0862e+01,  1.7866e+00,  1.2279e+01,  ...,  1.5264e+00,\n",
      "           1.8532e+00,  1.5831e+00],\n",
      "         [-1.3142e-01, -1.6170e+01,  3.1571e+00,  ..., -1.6274e+01,\n",
      "          -1.6147e+01, -1.6173e+01],\n",
      "         [-2.8254e+01, -4.3997e+01, -3.0093e+01,  ..., -4.3940e+01,\n",
      "          -4.3942e+01, -4.3656e+01],\n",
      "         ...,\n",
      "         [ 2.3286e+01,  8.0216e+00,  2.4295e+01,  ...,  7.8935e+00,\n",
      "           7.9091e+00,  7.6390e+00],\n",
      "         [ 2.3280e+01,  8.0094e+00,  2.4286e+01,  ...,  7.8815e+00,\n",
      "           7.8969e+00,  7.6268e+00],\n",
      "         [ 2.3274e+01,  7.9977e+00,  2.4277e+01,  ...,  7.8700e+00,\n",
      "           7.8854e+00,  7.6152e+00]],\n",
      "\n",
      "        [[ 9.1561e+00, -1.6158e+00,  8.2528e+00,  ..., -1.6192e+00,\n",
      "          -1.3569e+00, -1.7064e+00],\n",
      "         [ 2.2054e+00, -8.2905e+00,  5.3825e+00,  ..., -8.3792e+00,\n",
      "          -8.2773e+00, -8.4653e+00],\n",
      "         [-1.8309e+00, -1.6395e+01, -1.8554e+00,  ..., -1.6750e+01,\n",
      "          -1.6584e+01, -1.6632e+01],\n",
      "         ...,\n",
      "         [ 2.3030e+01,  7.4472e+00,  2.3840e+01,  ...,  7.3336e+00,\n",
      "           7.3431e+00,  7.0679e+00],\n",
      "         [ 2.3030e+01,  7.4469e+00,  2.3840e+01,  ...,  7.3333e+00,\n",
      "           7.3428e+00,  7.0677e+00],\n",
      "         [ 2.3030e+01,  7.4467e+00,  2.3840e+01,  ...,  7.3331e+00,\n",
      "           7.3426e+00,  7.0675e+00]],\n",
      "\n",
      "        [[ 1.8963e+00, -7.8482e+00,  6.4697e+00,  ..., -7.8853e+00,\n",
      "          -7.9656e+00, -7.9075e+00],\n",
      "         [-2.5684e+01, -3.6821e+01, -1.9591e+01,  ..., -3.6666e+01,\n",
      "          -3.6696e+01, -3.6346e+01],\n",
      "         [-1.3538e+00, -1.7750e+01,  1.0655e+00,  ..., -1.7538e+01,\n",
      "          -1.7705e+01, -1.7835e+01],\n",
      "         ...,\n",
      "         [ 1.4737e+01,  1.0547e+01,  2.3161e+01,  ...,  1.0631e+01,\n",
      "           1.0650e+01,  1.0285e+01],\n",
      "         [ 3.4996e+01,  2.1365e+01,  4.7880e+01,  ...,  2.1397e+01,\n",
      "           2.1146e+01,  2.0881e+01],\n",
      "         [ 1.2153e+01,  1.4247e+00,  1.9874e+01,  ...,  1.6105e+00,\n",
      "           1.4946e+00,  1.4008e+00]],\n",
      "\n",
      "        [[-1.2595e-02, -1.4600e+01,  5.5078e+00,  ..., -1.4913e+01,\n",
      "          -1.4738e+01, -1.4851e+01],\n",
      "         [-2.5848e+01, -3.4923e+01, -2.2910e+01,  ..., -3.4912e+01,\n",
      "          -3.4942e+01, -3.4690e+01],\n",
      "         [-7.3750e-01, -1.2478e+01,  1.8724e+00,  ..., -1.2608e+01,\n",
      "          -1.2579e+01, -1.2596e+01],\n",
      "         ...,\n",
      "         [ 2.3005e+01,  7.4167e+00,  2.3819e+01,  ...,  7.3052e+00,\n",
      "           7.3141e+00,  7.0379e+00],\n",
      "         [ 2.3004e+01,  7.4165e+00,  2.3819e+01,  ...,  7.3051e+00,\n",
      "           7.3140e+00,  7.0377e+00],\n",
      "         [ 2.3004e+01,  7.4164e+00,  2.3819e+01,  ...,  7.3050e+00,\n",
      "           7.3139e+00,  7.0377e+00]],\n",
      "\n",
      "        [[ 1.2542e+01,  5.3502e+00,  1.3590e+01,  ...,  5.4216e+00,\n",
      "           5.4198e+00,  5.2315e+00],\n",
      "         [-2.5598e-01, -1.5547e+01,  2.7342e+00,  ..., -1.5499e+01,\n",
      "          -1.5769e+01, -1.5603e+01],\n",
      "         [-2.5012e+01, -3.9589e+01, -2.0256e+01,  ..., -3.9341e+01,\n",
      "          -3.9452e+01, -3.9351e+01],\n",
      "         ...,\n",
      "         [ 2.3019e+01,  7.4314e+00,  2.3828e+01,  ...,  7.3189e+00,\n",
      "           7.3280e+00,  7.0523e+00],\n",
      "         [ 2.3019e+01,  7.4313e+00,  2.3828e+01,  ...,  7.3187e+00,\n",
      "           7.3279e+00,  7.0522e+00],\n",
      "         [ 2.3019e+01,  7.4311e+00,  2.3828e+01,  ...,  7.3185e+00,\n",
      "           7.3277e+00,  7.0520e+00]]], device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd199371a60>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[ 12.5423,   5.3503,  13.5899,  ...,   5.4216,   5.4198,   5.2315],\n",
      "         [ -0.2560, -15.5469,   2.7342,  ..., -15.4986, -15.7689, -15.6029],\n",
      "         [-25.0116, -39.5893, -20.2559,  ..., -39.3413, -39.4516, -39.3508],\n",
      "         ...,\n",
      "         [ 23.2085,   7.8715,  24.1782,  ...,   7.7465,   7.7606,   7.4900],\n",
      "         [ 23.2052,   7.8648,  24.1729,  ...,   7.7400,   7.7540,   7.4833],\n",
      "         [ 23.2019,   7.8583,  24.1677,  ...,   7.7336,   7.7475,   7.4769]],\n",
      "\n",
      "        [[-25.4574, -39.6478, -23.6803,  ..., -39.3751, -39.6624, -39.0897],\n",
      "         [-29.6836, -40.9928, -30.8316,  ..., -40.9621, -41.2014, -40.7481],\n",
      "         [ 21.4355,   8.2683,  21.4431,  ...,   7.8474,   8.0409,   7.9525],\n",
      "         ...,\n",
      "         [ 23.1635,   7.7800,  24.1061,  ...,   7.6564,   7.6697,   7.3991],\n",
      "         [ 23.1612,   7.7754,  24.1024,  ...,   7.6519,   7.6651,   7.3945],\n",
      "         [ 23.1590,   7.7708,  24.0987,  ...,   7.6474,   7.6606,   7.3899]],\n",
      "\n",
      "        [[ -1.2213, -16.0864,   1.8962,  ..., -16.1220, -16.1892, -16.0167],\n",
      "         [-15.7084, -25.1166, -13.3214,  ..., -24.9006, -24.8047, -24.9589],\n",
      "         [  8.5199,  -6.6704,   9.7536,  ...,  -6.7725,  -6.5894,  -6.6938],\n",
      "         ...,\n",
      "         [ 23.0575,   7.5195,  23.8939,  ...,   7.4028,   7.4133,   7.1399],\n",
      "         [ 23.0573,   7.5187,  23.8933,  ...,   7.4020,   7.4125,   7.1391],\n",
      "         [ 23.0571,   7.5179,  23.8926,  ...,   7.4012,   7.4117,   7.1382]],\n",
      "\n",
      "        [[  9.1561,  -1.6158,   8.2527,  ...,  -1.6192,  -1.3569,  -1.7064],\n",
      "         [ 11.3422,  -3.6371,  12.8073,  ...,  -3.6755,  -3.6585,  -3.7302],\n",
      "         [  8.1262,  -7.9547,   8.7234,  ...,  -8.0607,  -7.9606,  -8.0684],\n",
      "         ...,\n",
      "         [-10.4752, -19.4872,  -7.7255,  ..., -19.6099, -19.4477, -19.4278],\n",
      "         [ 14.1950,   3.0608,  23.1098,  ...,   3.0537,   3.1070,   2.8676],\n",
      "         [-15.2430, -32.3239, -22.2938,  ..., -32.4468, -32.3255, -32.4566]],\n",
      "\n",
      "        [[ 11.4206,   2.4968,  11.4456,  ...,   2.6206,   2.7765,   2.3511],\n",
      "         [-25.7196, -39.2122, -20.6823,  ..., -39.1387, -38.8004, -38.7339],\n",
      "         [-18.8742, -29.2107, -12.1899,  ..., -28.9453, -28.9708, -28.7754],\n",
      "         ...,\n",
      "         [ 23.1957,   7.8500,  24.1622,  ...,   7.7255,   7.7394,   7.4687],\n",
      "         [ 23.1925,   7.8437,  24.1572,  ...,   7.7193,   7.7331,   7.4624],\n",
      "         [ 23.1895,   7.8375,  24.1524,  ...,   7.7132,   7.7271,   7.4563]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd0a0b32640>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[-11.1971, -26.3159,  -8.8446,  ..., -26.5093, -26.3682, -26.1465],\n",
      "         [-25.9879, -37.8444, -27.9494,  ..., -37.8542, -37.9238, -37.7866],\n",
      "         [-12.3711, -21.8605,  -8.6800,  ..., -22.1802, -22.0219, -22.0401],\n",
      "         ...,\n",
      "         [ 23.0166,   7.4273,  23.8271,  ...,   7.3150,   7.3241,   7.0483],\n",
      "         [ 23.0165,   7.4271,  23.8270,  ...,   7.3149,   7.3240,   7.0482],\n",
      "         [ 23.0164,   7.4270,  23.8269,  ...,   7.3148,   7.3239,   7.0481]],\n",
      "\n",
      "        [[-25.4574, -39.6479, -23.6804,  ..., -39.3752, -39.6625, -39.0896],\n",
      "         [ -8.8103, -23.7147, -12.5231,  ..., -23.3790, -23.7959, -23.5833],\n",
      "         [-24.7066, -41.5925, -31.1057,  ..., -41.6090, -41.8103, -41.3409],\n",
      "         ...,\n",
      "         [ 25.6595,  19.1551,  35.2274,  ...,  18.9925,  18.7225,  18.6096],\n",
      "         [ 28.1184,  17.6725,  41.2017,  ...,  17.8887,  17.5955,  17.2734],\n",
      "         [ 29.3926,  15.2736,  24.2985,  ...,  14.8760,  14.9507,  14.6979]],\n",
      "\n",
      "        [[  1.3981,  -8.1870,   1.9304,  ...,  -8.0798,  -8.2595,  -8.3618],\n",
      "         [  3.6575,  -8.9830,   4.8426,  ...,  -8.9504,  -8.7884,  -9.3692],\n",
      "         [  0.5569, -10.0765,   1.7060,  ..., -10.0141, -10.0683, -10.2498],\n",
      "         ...,\n",
      "         [ 23.6184,   8.7177,  24.7556,  ...,   8.5759,   8.6003,   8.3300],\n",
      "         [ 23.5890,   8.6575,  24.7183,  ...,   8.5169,   8.5405,   8.2704],\n",
      "         [ 23.5625,   8.6024,  24.6838,  ...,   8.4630,   8.4859,   8.2158]],\n",
      "\n",
      "        [[-31.2409, -44.5738, -27.9620,  ..., -44.4697, -44.5899, -44.3269],\n",
      "         [-29.2787, -41.5158, -30.3491,  ..., -41.5449, -41.4716, -41.3283],\n",
      "         [ 11.2005,   2.3580,  12.0101,  ...,   2.2069,   2.1868,   2.2278],\n",
      "         ...,\n",
      "         [ 23.0852,   7.5979,  23.9552,  ...,   7.4786,   7.4901,   7.2180],\n",
      "         [ 23.0846,   7.5962,  23.9538,  ...,   7.4769,   7.4884,   7.2162],\n",
      "         [ 23.0839,   7.5944,  23.9523,  ...,   7.4752,   7.4866,   7.2144]],\n",
      "\n",
      "        [[-32.4816, -44.9646, -29.7490,  ..., -44.7881, -44.7429, -44.7196],\n",
      "         [-10.5856, -19.7320, -10.1924,  ..., -19.5705, -19.4793, -19.3464],\n",
      "         [-17.6524, -31.7166, -14.4056,  ..., -31.3397, -31.4566, -31.4589],\n",
      "         ...,\n",
      "         [ 23.0499,   7.5084,  23.8862,  ...,   7.3920,   7.4024,   7.1288],\n",
      "         [ 23.0496,   7.5075,  23.8855,  ...,   7.3912,   7.4016,   7.1279],\n",
      "         [ 23.0493,   7.5067,  23.8849,  ...,   7.3904,   7.4008,   7.1271]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd0a1bf8190>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[  0.3469, -13.6911,   4.6650,  ..., -13.7872, -13.7718, -13.7623],\n",
      "         [-21.1873, -34.7263, -13.1365,  ..., -34.3977, -34.5151, -34.5008],\n",
      "         [-23.8047, -37.1550, -22.1893,  ..., -36.9268, -36.9773, -36.8837],\n",
      "         ...,\n",
      "         [ 23.0529,   7.5210,  23.8962,  ...,   7.4042,   7.4149,   7.1414],\n",
      "         [ 23.0526,   7.5200,  23.8954,  ...,   7.4033,   7.4139,   7.1404],\n",
      "         [ 23.0523,   7.5191,  23.8946,  ...,   7.4024,   7.4130,   7.1395]],\n",
      "\n",
      "        [[-32.3283, -45.3312, -29.2714,  ..., -45.1931, -45.3169, -44.9916],\n",
      "         [  6.2348,  -9.5334,  11.0125,  ...,  -9.4526,  -9.5858,  -9.4254],\n",
      "         [-29.6454, -40.9429, -29.7609,  ..., -40.7007, -40.8495, -40.5746],\n",
      "         ...,\n",
      "         [ 23.4329,   8.3268,  24.5092,  ...,   8.1927,   8.2117,   7.9418],\n",
      "         [ 23.4181,   8.2964,  24.4888,  ...,   8.1630,   8.1816,   7.9118],\n",
      "         [ 23.4042,   8.2682,  24.4695,  ...,   8.1352,   8.1536,   7.8837]],\n",
      "\n",
      "        [[-16.1015, -30.1359, -14.8562,  ..., -30.1392, -29.8177, -29.8586],\n",
      "         [-21.6932, -37.9226, -19.6973,  ..., -37.7648, -37.9431, -37.6242],\n",
      "         [ 17.5242,   4.8930,  19.8926,  ...,   4.8966,   4.6583,   4.7488],\n",
      "         ...,\n",
      "         [ 23.1036,   7.6448,  23.9951,  ...,   7.5244,   7.5364,   7.2648],\n",
      "         [ 23.1025,   7.6424,  23.9931,  ...,   7.5220,   7.5341,   7.2624],\n",
      "         [ 23.1015,   7.6401,  23.9912,  ...,   7.5197,   7.5317,   7.2600]],\n",
      "\n",
      "        [[  1.8345,  -9.7733,   1.6399,  ...,  -9.9342,  -9.9595,  -9.7913],\n",
      "         [ -7.1145, -20.0487,  -0.1193,  ..., -19.9467, -20.1043, -20.1689],\n",
      "         [ 12.6587,   4.6920,  19.3706,  ...,   4.7344,   4.8987,   4.7078],\n",
      "         ...,\n",
      "         [ 23.2665,   7.9921,  24.2735,  ...,   7.8647,   7.8801,   7.6098],\n",
      "         [ 23.2607,   7.9807,  24.2650,  ...,   7.8536,   7.8688,   7.5985],\n",
      "         [ 23.2552,   7.9698,  24.2568,  ...,   7.8429,   7.8580,   7.5877]],\n",
      "\n",
      "        [[  3.7175, -10.6185,   5.4207,  ..., -10.4998, -10.4502, -10.6867],\n",
      "         [-29.0377, -40.7875, -28.2647,  ..., -40.7847, -40.7842, -40.6408],\n",
      "         [ 11.5857,   1.1048,  13.0148,  ...,   1.0165,   1.2066,   0.8276],\n",
      "         ...,\n",
      "         [  9.9838,  -1.1892,  15.9663,  ...,  -1.0745,  -1.2236,  -1.0470],\n",
      "         [ 26.1361,  15.6988,  34.8620,  ...,  15.7640,  15.7522,  15.5104],\n",
      "         [ -9.8601, -23.6963, -13.3030,  ..., -23.7893, -23.6560, -23.9819]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd19a400670>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[-24.6006, -36.2535, -23.3919,  ..., -36.1360, -36.1098, -35.8468],\n",
      "         [  5.5104,  -4.1268,  10.2738,  ...,  -4.2632,  -4.4420,  -4.1876],\n",
      "         [  8.6148,  -0.9257,  12.5652,  ...,  -1.0140,  -1.3156,  -0.9746],\n",
      "         ...,\n",
      "         [ 23.0619,   7.5360,  23.9069,  ...,   7.4187,   7.4295,   7.1564],\n",
      "         [ 23.0616,   7.5350,  23.9060,  ...,   7.4177,   7.4285,   7.1554],\n",
      "         [ 23.0613,   7.5339,  23.9052,  ...,   7.4167,   7.4275,   7.1543]],\n",
      "\n",
      "        [[-25.4574, -39.6479, -23.6804,  ..., -39.3752, -39.6625, -39.0896],\n",
      "         [-29.6835, -40.9928, -30.8315,  ..., -40.9621, -41.2014, -40.7481],\n",
      "         [-19.3658, -31.6956, -10.1740,  ..., -31.3305, -31.4952, -31.1081],\n",
      "         ...,\n",
      "         [  9.6929,   1.5452,  13.9760,  ...,   1.6542,   1.3662,   1.6088],\n",
      "         [ -6.9970, -19.6664,   2.4844,  ..., -19.6165, -19.5083, -19.5124],\n",
      "         [ -7.5359, -18.1326, -10.7451,  ..., -18.2396, -18.0590, -18.2552]],\n",
      "\n",
      "        [[  2.2013,  -8.3857,   1.3825,  ...,  -8.5157,  -8.5626,  -8.5299],\n",
      "         [  8.7708,   1.6142,  11.4777,  ...,   1.5190,   1.6729,   1.6157],\n",
      "         [-18.0910, -27.7630, -13.9768,  ..., -27.6548, -27.7352, -27.2497],\n",
      "         ...,\n",
      "         [ 23.1464,   7.7524,  24.0838,  ...,   7.6300,   7.6430,   7.3716],\n",
      "         [ 23.1445,   7.7483,  24.0805,  ...,   7.6259,   7.6389,   7.3675],\n",
      "         [ 23.1425,   7.7442,  24.0771,  ...,   7.6219,   7.6348,   7.3635]],\n",
      "\n",
      "        [[-16.1015, -30.1359, -14.8562,  ..., -30.1392, -29.8177, -29.8586],\n",
      "         [ 12.6016,  -1.0599,  17.9247,  ...,  -0.9797,  -1.0453,  -0.8296],\n",
      "         [-22.2309, -36.0621, -17.8011,  ..., -36.2182, -36.3104, -35.7322],\n",
      "         ...,\n",
      "         [ 23.0451,   7.4964,  23.8772,  ...,   7.3806,   7.3909,   7.1168],\n",
      "         [ 23.0449,   7.4957,  23.8767,  ...,   7.3800,   7.3902,   7.1162],\n",
      "         [ 23.0447,   7.4950,  23.8762,  ...,   7.3793,   7.3896,   7.1155]],\n",
      "\n",
      "        [[-25.4574, -39.6479, -23.6804,  ..., -39.3752, -39.6625, -39.0896],\n",
      "         [ -1.2977, -14.6116,   1.4648,  ..., -14.6119, -14.8867, -14.8307],\n",
      "         [ -1.3544, -19.0343,  -1.5352,  ..., -19.0886, -18.9327, -19.1788],\n",
      "         ...,\n",
      "         [ 23.0326,   7.4569,  23.8482,  ...,   7.3429,   7.3527,   7.0777],\n",
      "         [ 23.0324,   7.4566,  23.8480,  ...,   7.3426,   7.3523,   7.0774],\n",
      "         [ 23.0323,   7.4563,  23.8477,  ...,   7.3423,   7.3520,   7.0770]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd19937f3d0>, hidden_states=None)\n",
      "MambaCausalLMOutput(loss=None, logits=tensor([[[  0.7556,  -9.6681,   1.6805,  ...,  -9.6642,  -9.6779,  -9.5914],\n",
      "         [-30.3084, -41.0670, -26.2149,  ..., -40.9761, -40.9125, -40.4896],\n",
      "         [  7.9724,  -0.8455,   8.7935,  ...,  -0.8616,  -1.0639,  -0.8060],\n",
      "         ...,\n",
      "         [ 23.0378,   7.4864,  23.8696,  ...,   7.3709,   7.3810,   7.1070],\n",
      "         [ 23.0376,   7.4858,  23.8690,  ...,   7.3703,   7.3804,   7.1064],\n",
      "         [ 23.0374,   7.4851,  23.8685,  ...,   7.3696,   7.3797,   7.1057]],\n",
      "\n",
      "        [[  2.2013,  -8.3857,   1.3825,  ...,  -8.5157,  -8.5626,  -8.5299],\n",
      "         [  7.0713,   2.8041,   9.7732,  ...,   2.7762,   2.8261,   2.6992],\n",
      "         [-28.0633, -37.0090, -23.7230,  ..., -36.8427, -36.9333, -36.5867],\n",
      "         ...,\n",
      "         [ 24.6070,  10.1677,  25.6636,  ...,   9.9942,  10.0357,   9.7657],\n",
      "         [ 23.9243,   9.2247,  25.0818,  ...,   9.0748,   9.1033,   8.8314],\n",
      "         [ 23.8753,   9.1358,  25.0022,  ...,   8.9846,   9.0136,   8.7422]],\n",
      "\n",
      "        [[ 13.8232,   3.7820,  12.2585,  ...,   3.5773,   3.8909,   3.5437],\n",
      "         [  4.3073,  -4.0629,   6.8025,  ...,  -4.2143,  -4.0122,  -4.2234],\n",
      "         [ -8.2519, -19.4314,  -5.2052,  ..., -19.6303, -19.3130, -19.3946],\n",
      "         ...,\n",
      "         [ 23.0483,   7.4754,  23.8579,  ...,   7.3598,   7.3697,   7.0958],\n",
      "         [ 23.0482,   7.4749,  23.8576,  ...,   7.3593,   7.3693,   7.0953],\n",
      "         [ 23.0481,   7.4745,  23.8573,  ...,   7.3589,   7.3688,   7.0949]],\n",
      "\n",
      "        [[-25.3266, -39.8049, -24.9164,  ..., -39.6572, -39.4906, -39.5425],\n",
      "         [-30.8097, -45.5684, -29.9234,  ..., -45.3914, -45.2775, -45.1926],\n",
      "         [-31.5204, -45.1924, -25.7380,  ..., -44.8985, -44.9028, -44.6842],\n",
      "         ...,\n",
      "         [ -1.6844, -12.0662,   2.1075,  ..., -12.0639, -12.0218, -11.9395],\n",
      "         [ 34.0176,  20.6345,  40.0476,  ...,  20.5846,  20.6529,  20.5300],\n",
      "         [ 32.2796,  19.8420,  29.8960,  ...,  19.8442,  19.8860,  19.6341]]],\n",
      "       device='cuda:2'), cache_params=<transformers.models.mamba.modeling_mamba.MambaCache object at 0x7fd19937f250>, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range((len(dataset[\"test\"][\"Text\"]) // 5) + 1):\n",
    "        batch = dataset[\"test\"][\"Text\"][i*5:(1+i)*5]\n",
    "        encodings = tokenizer(batch, return_tensors= \"pt\", padding=True)\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        outputs = model(input_ids)\n",
    "        print(tokenizer.decode(outputs.logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.0040,  -5.3652,   4.8236,  ...,  -5.4267,  -5.4573,  -5.4511],\n",
       "         [  2.5265,  -5.9306,   4.7795,  ...,  -5.8653,  -5.9365,  -6.1129],\n",
       "         [  8.4780,  -0.7837,   8.8055,  ...,  -0.8313,  -0.8446,  -0.9851],\n",
       "         ...,\n",
       "         [ 23.1579,   7.7685,  24.0984,  ...,   7.6454,   7.6587,   7.3877],\n",
       "         [ 23.1557,   7.7640,  24.0947,  ...,   7.6410,   7.6542,   7.3833],\n",
       "         [ 23.1536,   7.7597,  24.0911,  ...,   7.6367,   7.6499,   7.3789]],\n",
       "\n",
       "        [[ -1.5431, -16.3554,   3.8924,  ..., -16.5353, -16.3415, -16.4642],\n",
       "         [-30.1016, -43.8162, -32.0511,  ..., -43.7683, -43.7739, -43.4250],\n",
       "         [-29.4698, -42.7763, -24.9283,  ..., -42.6009, -42.3791, -42.2224],\n",
       "         ...,\n",
       "         [ 23.8009,   9.0419,  24.9545,  ...,   8.8931,   8.9214,   8.6503],\n",
       "         [ 23.7398,   8.9403,  24.8939,  ...,   8.7936,   8.8207,   8.5497],\n",
       "         [ 23.7064,   8.8789,  24.8581,  ...,   8.7336,   8.7599,   8.4893]],\n",
       "\n",
       "        [[-25.3489, -37.9731, -23.3293,  ..., -37.9446, -37.8954, -37.8251],\n",
       "         [-31.4593, -43.2287, -33.0063,  ..., -43.1930, -43.2536, -42.8974],\n",
       "         [-28.4309, -37.7938, -26.1109,  ..., -37.6267, -37.5906, -37.2472],\n",
       "         ...,\n",
       "         [ 23.0230,   7.4351,  23.8303,  ...,   7.3219,   7.3311,   7.0560],\n",
       "         [ 23.0229,   7.4349,  23.8302,  ...,   7.3217,   7.3309,   7.0558],\n",
       "         [ 23.0227,   7.4348,  23.8301,  ...,   7.3216,   7.3308,   7.0556]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  2.2013,  -8.3857,   1.3825,  ...,  -8.5157,  -8.5626,  -8.5299],\n",
       "         [  7.0713,   2.8041,   9.7732,  ...,   2.7762,   2.8261,   2.6992],\n",
       "         [-28.0633, -37.0090, -23.7229,  ..., -36.8427, -36.9333, -36.5866],\n",
       "         ...,\n",
       "         [ 23.3260,   8.1095,  24.3581,  ...,   7.9799,   7.9963,   7.7263],\n",
       "         [ 23.3176,   8.0925,  24.3460,  ...,   7.9631,   7.9794,   7.7093],\n",
       "         [ 23.3095,   8.0763,  24.3344,  ...,   7.9472,   7.9633,   7.6932]],\n",
       "\n",
       "        [[ 13.8232,   3.7820,  12.2586,  ...,   3.5772,   3.8908,   3.5436],\n",
       "         [  4.3073,  -4.0629,   6.8025,  ...,  -4.2143,  -4.0121,  -4.2234],\n",
       "         [ -8.2518, -19.4314,  -5.2051,  ..., -19.6302, -19.3130, -19.3946],\n",
       "         ...,\n",
       "         [ 23.0446,   7.4656,  23.8507,  ...,   7.3504,   7.3602,   7.0860],\n",
       "         [ 23.0444,   7.4652,  23.8505,  ...,   7.3501,   7.3599,   7.0857],\n",
       "         [ 23.0442,   7.4648,  23.8502,  ...,   7.3498,   7.3595,   7.0853]],\n",
       "\n",
       "        [[-25.3267, -39.8050, -24.9164,  ..., -39.6573, -39.4906, -39.5425],\n",
       "         [-30.8097, -45.5683, -29.9234,  ..., -45.3913, -45.2774, -45.1926],\n",
       "         [-31.5204, -45.1924, -25.7380,  ..., -44.8985, -44.9029, -44.6842],\n",
       "         ...,\n",
       "         [ 23.3620,   8.1870,  24.4108,  ...,   8.0563,   8.0734,   7.8034],\n",
       "         [ 23.3513,   8.1654,  24.3958,  ...,   8.0351,   8.0520,   7.7821],\n",
       "         [ 23.3412,   8.1449,  24.3816,  ...,   8.0150,   8.0317,   7.7617]]],\n",
       "       device='cuda:2')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perplexity.compute(predictions=predictions, model_id='gpt2', add_start_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
