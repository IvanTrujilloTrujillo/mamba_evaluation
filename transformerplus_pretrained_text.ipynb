{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Trelis/tiny-shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (472, 1), 'test': (49, 1)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text'],\n",
       "    num_rows: 472\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TRANIO:\\nIs this your speeding? nay, then, good night our part!\\n\\nPETRUCHIO:\\nBe patient, gentlemen; I choose her for myself:\\nIf she and I be pleased, what's that to you?\\n'Tis bargain'd 'twixt us twain, being alone,\\nThat she shall still be curst in company.\\nI tell you, 'tis incredible to believe\\nHow much she loves me: O, the kindest Kate!\\nShe hung about my neck; and kiss on kiss\\nShe vied so fast, protesting oath on oath,\\nThat in a twink she won me to her love.\\nO, you are novices! 'tis a world to see,\\nHow tame, when men and women are alone,\\nA meacock wretch can make the curstest shrew.\\nGive me thy hand, Kate: I will unto Venice,\\nTo buy apparel 'gainst the wedding-day.\\nProvide the feast, father, and bid the guests;\\nI will be sure my Katharina shall be fine.\\n\\nBAPTISTA:\\nI know not what to say: but give me your hands;\\nGod send you joy, Petruchio! 'tis a match.\\n\\nGREMIO:\\nAmen, say we: we will be witnesses.\\n\\nPETRUCHIO:\\nFather, and wife, and gentlemen, adieu;\\nI will to Venice; Sunday comes apace:\\nWe will have rings and things and fine array;\\nAnd kiss me, Kate, we will be married o'Sunday.\\n\\nGREMIO:\\nWas ever match clapp'd up so suddenly?\\n\\nBAPTISTA:\\nFaith, gentlemen, now I play a merchant's part,\\nAnd venture madly on a desperate mart.\\n\\nTRANIO:\\n'Twas a commodity lay fretting by you:\\n'Twill bring you gain, or perish on the seas.\\n\\nBAPTISTA:\\nThe gain I seek is, quiet in the match.\\n\\nGREMIO:\\nNo doubt but he hath got a quiet catch.\\nBut now, Baptists, to your younger daughter:\\nNow is the day we long have looked for:\\nI am your neighbour, and was suitor first.\\n\\nTRANIO:\\nAnd I am one that love Bianca more\\nThan words can witness, or your thoughts can guess.\\n\\nGREMIO:\\nYoungling, thou canst not love so dear as I.\\n\\nTRANIO:\\nGraybeard, thy love doth freeze.\\n\\nGREMIO:\\nBut thine doth fry.\\nSkipper, stand back: 'tis age that nourisheth.\\n\\nTRANIO:\\nBut youth in ladies' eyes that flourisheth.\\n\\nBAPTISTA:\\nContent you, gentlemen: I will compound this strife:\\n'Tis deeds must win the prize; and he of both\\nThat can assure my daughter greatest dower\\nShall have my Bianca's love.\\nSay, Signior Gremio, What can you assure her?\\n\\nGREMIO:\\nFirst, as you know, my house within the city\\nIs richly furnished with plate and gold;\\nBasins and ewers to lave her dainty hands;\\nMy hangings all of Tyrian tapestry;\\nIn ivory coffers I have stuff'd my crowns;\\nIn cypress chests my arras counterpoints,\\nCostly apparel, tents, and canopies,\\nFine linen, Turkey cushions boss'd with pearl,\\nValance of Venice gold in needlework,\\nPewter and brass and all things that belong\\nTo house or housekeeping: then, at my farm\\nI have a hundred milch-kine to the pail,\\nSixscore fat oxen standing in my stalls,\\nAnd all things answerable to this portion.\\nMyself am struck in years, I must confess;\\nAnd if I die to-morrow, this is hers,\\nIf whilst I live she will be only mine.\\n\\nTRANIO:\\nThat 'only' came well in.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2859"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"test\"][\"Text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"8B\"\n",
    "# model_size = \"70B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43fb3c5b2c047fdada9129019236d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"meta-llama/Meta-Llama-3-{model_size}\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"meta-llama/Meta-Llama-3-{model_size}\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(dataset[\"test\"][\"Text\"], return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itrujillo/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'architectures': ['LlamaForCausalLM'],\n",
       "  'attention_bias': False,\n",
       "  'attention_dropout': 0.0,\n",
       "  'bos_token_id': 128000,\n",
       "  'eos_token_id': 128001,\n",
       "  'hidden_act': 'silu',\n",
       "  'hidden_size': 4096,\n",
       "  'initializer_range': 0.02,\n",
       "  'intermediate_size': 14336,\n",
       "  'max_position_embeddings': 8192,\n",
       "  'model_type': 'llama',\n",
       "  'num_attention_heads': 32,\n",
       "  'num_hidden_layers': 32,\n",
       "  'num_key_value_heads': 8,\n",
       "  'pretraining_tp': 1,\n",
       "  'rms_norm_eps': 1e-05,\n",
       "  'rope_scaling': None,\n",
       "  'rope_theta': 500000.0,\n",
       "  'tie_word_embeddings': False,\n",
       "  'torch_dtype': 'bfloat16',\n",
       "  'transformers_version': '4.40.0.dev0',\n",
       "  'use_cache': True,\n",
       "  'vocab_size': 128256,\n",
       "  '_commit_hash': '62bd457b6fe961a42a631306577e622c83876cb6'},\n",
       " {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.get_config_dict(f\"meta-llama/Meta-Llama-3-{model_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB (GPU 2; 31.75 GiB total capacity; 30.25 GiB already allocated; 19.50 MiB free; 30.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-15de8d192bfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencodings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1165\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    966\u001b[0m                 )\n\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    969\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB (GPU 2; 31.75 GiB total capacity; 30.25 GiB already allocated; 19.50 MiB free; 30.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    model = model.to(device)\n",
    "    for text in dataset[\"test\"][\"Text\"]:\n",
    "        encodings = tokenizer(text, return_tensors= \"pt\", padding=True)\n",
    "        input_ids = encodings.input_ids.to(device)\n",
    "        outputs = model(input_ids)\n",
    "        p = tokenizer.decode(outputs.logits.argmax(dim=-1)[0], skip_special_tokens=True)\n",
    "        predictions.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"TRANIO:\\nIs this your speeding? nay, then, good night our part!\\n\\nPETRUCHIO:\\nBe patient, gentlemen; I choose her for myself:\\nIf she and I be pleased, what's that to you?\\n'Tis bargain'd 'twixt us twain, being alone,\\nThat she shall still be curst in company.\\nI tell you, 'tis incredible to believe\\nHow much she loves me: O, the kindest Kate!\\nShe hung about my neck; and kiss on kiss\\nShe vied so fast, protesting oath on oath,\\nThat in a twink she won me to her love.\\nO, you are novices! 'tis a world to see,\\nHow tame, when men and women are alone,\\nA meacock wretch can make the curstest shrew.\\nGive me thy hand, Kate: I will unto Venice,\\nTo buy apparel 'gainst the wedding-day.\\nProvide the feast, father, and bid the guests;\\nI will be sure my Katharina shall be fine.\\n\\nBAPTISTA:\\nI know not what to say: but give me your hands;\\nGod send you joy, Petruchio! 'tis a match.\\n\\nGREMIO:\\nAmen, say we: we will be witnesses.\\n\\nPETRUCHIO:\\nFather, and wife, and gentlemen, adieu;\\nI will to Venice; Sunday comes apace:\\nWe will have rings and things and fine array;\\nAnd kiss me, Kate, we will be married o'Sunday.\\n\\nGREMIO:\\nWas ever match clapp'd up so suddenly?\\n\\nBAPTISTA:\\nFaith, gentlemen, now I play a merchant's part,\\nAnd venture madly on a desperate mart.\\n\\nTRANIO:\\n'Twas a commodity lay fretting by you:\\n'Twill bring you gain, or perish on the seas.\\n\\nBAPTISTA:\\nThe gain I seek is, quiet in the match.\\n\\nGREMIO:\\nNo doubt but he hath got a quiet catch.\\nBut now, Baptists, to your younger daughter:\\nNow is the day we long have looked for:\\nI am your neighbour, and was suitor first.\\n\\nTRANIO:\\nAnd I am one that love Bianca more\\nThan words can witness, or your thoughts can guess.\\n\\nGREMIO:\\nYoungling, thou canst not love so dear as I.\\n\\nTRANIO:\\nGraybeard, thy love doth freeze.\\n\\nGREMIO:\\nBut thine doth fry.\\nSkipper, stand back: 'tis age that nourisheth.\\n\\nTRANIO:\\nBut youth in ladies' eyes that flourisheth.\\n\\nBAPTISTA:\\nContent you, gentlemen: I will compound this strife:\\n'Tis deeds must win the prize; and he of both\\nThat can assure my daughter greatest dower\\nShall have my Bianca's love.\\nSay, Signior Gremio, What can you assure her?\\n\\nGREMIO:\\nFirst, as you know, my house within the city\\nIs richly furnished with plate and gold;\\nBasins and ewers to lave her dainty hands;\\nMy hangings all of Tyrian tapestry;\\nIn ivory coffers I have stuff'd my crowns;\\nIn cypress chests my arras counterpoints,\\nCostly apparel, tents, and canopies,\\nFine linen, Turkey cushions boss'd with pearl,\\nValance of Venice gold in needlework,\\nPewter and brass and all things that belong\\nTo house or housekeeping: then, at my farm\\nI have a hundred milch-kine to the pail,\\nSixscore fat oxen standing in my stalls,\\nAnd all things answerable to this portion.\\nMyself am struck in years, I must confess;\\nAnd if I die to-morrow, this is hers,\\nIf whilst I live she will be only mine.\\n\\nTRANIO:\\nThat 'only' came well in.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  49873,   3895,    512,   3957,    420,    701,  58675,     30,\n",
       "         103091,     11,   1243,     11,   1695,   3814,   1057,    961,   2268,\n",
       "          80504,     49,  30593,   3895,    512,   3513,   8893,     11,  58909,\n",
       "             26,    358,   5268,   1077,    369,   7182,    512,   2746,   1364,\n",
       "            323,    358,    387,  18949,     11,   1148,    596,    430,    311,\n",
       "            499,   5380,  17773,    285,  45663,   4265,    364,  15930,    953,\n",
       "             83,    603,   4483,    467,     11,   1694,   7636,    345,   4897,\n",
       "           1364,   4985,   2103,    387,   2917,    267,    304,   2883,    627,\n",
       "             40,   3371,    499,     11,    364,     83,    285,  15400,    311,\n",
       "           4510,    198,   4438,   1790,   1364,  16180,    757,     25,    507,\n",
       "             11,    279,  24890,   5086,  30301,   4999,   8100,  18799,    922,\n",
       "            856,  13272,     26,    323,  21735,    389,  21735,    198,   8100,\n",
       "            348,   1142,    779,   5043,     11,  59310,  49042,    389,  49042,\n",
       "            345,   4897,    304,    264,  97500,   1364,   2834,    757,    311,\n",
       "           1077,   3021,    627,     46,     11,    499,    527,   6747,   1238,\n",
       "              0,    364,     83,    285,    264,   1917,    311,   1518,    345,\n",
       "           4438,  82923,     11,    994,   3026,    323,   3278,    527,   7636,\n",
       "            345,     32,    757,  81378,    289,  10420,    649,   1304,    279,\n",
       "           2917,    267,    478,    559,   4361,    627,  36227,    757,  26236,\n",
       "           1450,     11,  30301,     25,    358,    690,  30449,  56750,    345,\n",
       "           1271,   3780,  55425,    364,  60246,    267,    279,  13306,  11477,\n",
       "            627,  61524,    279,  53268,     11,   7126,     11,    323,  14435,\n",
       "            279,  15051,    280,     40,    690,    387,   2771,    856,  33995,\n",
       "            277,   2259,   4985,    387,   7060,    382,     33,  67062,  96965,\n",
       "            512,     40,   1440,    539,   1148,    311,   2019,     25,    719,\n",
       "           3041,    757,    701,   6206,    280,  28622,   3708,    499,  16267,\n",
       "             11,  96876,   1412,    822,      0,    364,     83,    285,    264,\n",
       "           2489,    382,  58163,     44,   3895,    512,     32,   5794,     11,\n",
       "           2019,    584,     25,    584,    690,    387,  28823,    382,  80504,\n",
       "             49,  30593,   3895,    512,  62416,     11,    323,   7555,     11,\n",
       "            323,  58909,     11,   1008,  26235,    280,     40,    690,    311,\n",
       "          56750,     26,   7418,   4131,    264,   1330,    512,   1687,    690,\n",
       "            617,  25562,    323,   2574,    323,   7060,   1358,    280,   3112,\n",
       "          21735,    757,     11,  30301,     11,    584,    690,    387,  12502,\n",
       "            297,  13575,   6815,    382,  58163,     44,   3895,    512,  27125,\n",
       "           3596,   2489,   1206,    680,   4265,    709,    779,  15187,   1980,\n",
       "             33,  67062,  96965,    512,  48334,    411,     11,  58909,     11,\n",
       "           1457,    358,   1514,    264,  30338,    596,    961,    345,   3112,\n",
       "          26255,  13088,    398,    389,    264,  28495,  49295,    382,  49873,\n",
       "           3895,    512,  17773,  16514,    264,  38983,  11203,  54164,   1303,\n",
       "            555,    499,    512,  17773,  14724,   4546,    499,   8895,     11,\n",
       "            477,  83217,    389,    279,  52840,    382,     33,  67062,  96965,\n",
       "            512,    791,   8895,    358,   6056,    374,     11,  11594,    304,\n",
       "            279,   2489,    382,  58163,     44,   3895,    512,   2822,  10712,\n",
       "            719,    568,  52677,   2751,    264,  11594,   2339,    627,   4071,\n",
       "           1457,     11,  35976,   1705,     11,    311,    701,  14992,  10003,\n",
       "            512,   7184,    374,    279,   1938,    584,   1317,    617,   7111,\n",
       "            369,    512,     40,   1097,    701,  22686,     11,    323,    574,\n",
       "            924,   1960,   1176,    382,  49873,   3895,    512,   3112,    358,\n",
       "           1097,    832,    430,   3021,  68045,    936,    810,    198,  27159,\n",
       "           4339,    649,  11550,     11,    477,    701,  11555,    649,   8101,\n",
       "            382,  58163,     44,   3895,    512,  41672,   2785,     11,  34223,\n",
       "            649,    267,    539,   3021,    779,  25237,    439,    358,    382,\n",
       "          49873,   3895,    512,  29274,   1395,    569,     11,  26236,   3021,\n",
       "            656,    339,  31030,    382,  58163,     44,   3895,    512,   4071,\n",
       "            270,    483,    656,    339,  53646,    627,  36234,    716,     11,\n",
       "           2559,   1203,     25,    364,     83,    285,   4325,    430,  46798,\n",
       "            819,    774,    382,  49873,   3895,    512,   4071,  12822,    304,\n",
       "          23628,      6,   6548,    430,  67784,    774,    382,     33,  67062,\n",
       "          96965,    512,   2831,    499,     11,  58909,     25,    358,    690,\n",
       "          24549,    420,  97712,    512,  17773,    285,  54811,   2011,   3243,\n",
       "            279,  22643,     26,    323,    568,    315,   2225,    198,   4897,\n",
       "            649,  36015,    856,  10003,  12474,    294,   1223,    198,   2059,\n",
       "            543,    617,    856,  68045,    936,    596,   3021,    627,  46864,\n",
       "             11,   7220,   2521,    480,   1864,    822,     11,   3639,    649,\n",
       "            499,  36015,   1077,   1980,  58163,     44,   3895,    512,   5451,\n",
       "             11,    439,    499,   1440,     11,    856,   3838,   2949,    279,\n",
       "           3363,    198,   3957,   9257,    398,  24330,    449,  12235,    323,\n",
       "           6761,    280,  34703,   1354,    323,  37990,    388,    311,    326,\n",
       "            525,   1077,    294,  18773,   6206,    280,   5159,  15020,    826,\n",
       "            682,    315,  14221,   7414,  17401,   4720,    280,    644,  70916,\n",
       "          10095,    388,    358,    617,   6392,   4265,    856,   9460,   4511,\n",
       "            280,    644,    272,  48701,  84418,    856,   2961,    300,   5663,\n",
       "           7862,    345,  15289,    398,  55425,     11,  64470,     11,    323,\n",
       "            649,  76641,    345,  64816,  55115,     11,  17442,  68241,  13697,\n",
       "           4265,    449,  70723,    345,   2257,    685,    315,  56750,   6761,\n",
       "            304,  31409,   1816,    345,     47,    365,    466,    323,  37138,\n",
       "            323,    682,   2574,    430,   9352,    198,   1271,   3838,    477,\n",
       "           3838,  33494,     25,   1243,     11,    520,    856,   8961,    198,\n",
       "             40,    617,    264,   7895,   7625,    331,  12934,    483,    311,\n",
       "            279,    281,    607,    345,  42560,  12618,   8834,  19488,    268,\n",
       "          11509,    304,    856,  74673,    345,   3112,    682,   2574,   4320,\n",
       "            481,    311,    420,  13651,    627,   5159,    726,   1097,  17948,\n",
       "            304,   1667,     11,    358,   2011,  48466,    280,   3112,    422,\n",
       "            358,   2815,    311,   1474,   7924,     11,    420,    374,  11074,\n",
       "            345,   2746,  24797,    358,   3974,   1364,    690,    387,   1193,\n",
       "          10705,    382,  49873,   3895,    512,   4897,    364,   3323,      6,\n",
       "           3782,   1664,    304,     13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(dataset[\"test\"][\"Text\"][0], return_tensors= \"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4ba503436d12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = perplexity.compute(predictions=predictions, model_id='gpt2', add_start_token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados perplexity:\n",
    "\n",
    "Â· Trelis/tiny-shakespeare (test):\n",
    "\n",
    "    8B ->  points\n",
    "    70B ->  points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
